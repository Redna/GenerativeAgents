import asyncio
import os

from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline

import re
from langchain import PromptTemplate
from langchain.llms import HuggingFaceEndpoint
from langchain.chains import LLMChain
import torch


from typing import Any, Dict, List, Mapping, Optional

import requests

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from langchain.pydantic_v1 import Extra, root_validator
from langchain.utils import get_from_dict_or_env
from langchain_openai import ChatOpenAI

VALID_TASKS = ("text2text-generation", "text-generation", "summarization")


class TransformersBatchInference(LLM):

    endpoint_url: str = ""
    """Endpoint URL to use."""

    model_kwargs: Optional[dict] = None
    """Key word arguments to pass to the model."""

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _model_kwargs = self.model_kwargs or {}
        return {
            **{"endpoint_url": self.endpoint_url},
            **{"model_kwargs": _model_kwargs},
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "huggingface_endpoint"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to HuggingFace Hub's inference endpoint.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = hf("Tell me a joke.")
        """
        _model_kwargs = self.model_kwargs or {}

        # payload samples
        params = {**_model_kwargs, **kwargs}
        parameter_payload = {"inputs": prompt, "parameters": params}

        # HTTP headers for authorization
        headers = {
            "Content-Type": "application/json",
        }

        try:
            response = requests.post(
                self.endpoint_url, headers=headers, json=parameter_payload
            )
        except requests.exceptions.RequestException as e:  # This is the correct syntax
            raise ValueError(f"Error raised by inference endpoint: {e}")
       
        payload = response.json()
        if "error" in payload:
            raise ValueError(
                f"Error raised by inference API: {payload['error']}"
            )
        
        text = payload["generated_text"][0]["generated_text"]
        if stop is not None:
            # This is a bit hacky, but I can't figure out a better way to enforce
            # stop tokens when making calls to huggingface_hub.
            text = enforce_stop_tokens(text, stop)
        return text


#llm = TransformersBatchInference(endpoint_url="http://localhost:30091/v1/generation")

llm = ChatOpenAI(openai_api_key="na", openai_api_base="http://localhost:8080/v1/")


async def __run():
    tasks = [llm.ainvoke("Tell me a joke."), llm.ainvoke("Tell me a good joke."), llm.ainvoke("Tell me a bad joke.")]
    return await asyncio.gather(*tasks)

async def __run_chain():

    _template = """<|system|> You write a concise description about {agent}'s personality, family situation and characteristics. You include ALL the details provided in the given context (you MUST include all the names of persons, ages,...)."""
    _prompt = PromptTemplate(input_variables=["agent", "identity"],
                            template=_template)
    
    _identity_chain = LLMChain(prompt=_prompt, llm=llm,
    verbose=True)

    x= await _identity_chain.arun(agent="agent", identity="identity")
    print(x)


async def __runall():
    r = await __run()
    print(r)
    await __run_chain()

if __name__ == "__main__":
    asyncio.run(__runall())
    pass